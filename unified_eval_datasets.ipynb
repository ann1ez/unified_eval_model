{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cea7e65",
   "metadata": {},
   "source": [
    "# Dataset Generation for Unified Evals Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0190d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4745e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==3.6.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (0.34.4)\n",
      "Requirement already satisfied: packaging in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from datasets==3.6.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85c2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anniezhu/Desktop/evrim/handshake_pipeline/.venv/lib/python3.12/site-packages/mlflow/__init__.py:41: UserWarning: Versions of mlflow (3.5.1) and child packages mlflow-skinny (3.8.1) are different. This may lead to unexpected behavior. Please install the same version of all MLflow packages.\n",
      "  mlflow.mismatch._check_version_mismatch()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "import mlflow\n",
    "from evaltune.evaluation.scorers import make_guidelines_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = random.Random(229)\n",
    "np.random.seed(229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fad5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./evals_benchmark_datasets\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b968f",
   "metadata": {},
   "source": [
    "# Ground Truth Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa6c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def save_minimal_dataset(df: pd.DataFrame, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Save df with exactly columns: input, output, label\n",
    "    \"\"\"\n",
    "    df = df[[\"input\", \"output\", \"label\"]].copy()\n",
    "    path_csv = os.path.join(OUT_DIR, f\"{name}.csv\")\n",
    "    df.to_csv(path_csv, index=False)\n",
    "    return path_csv\n",
    "\n",
    "def stratified_sample(df: pd.DataFrame, n: int, label_col: str = \"label\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try to sample n rows while keeping a reasonable mix of labels.\n",
    "    If one class is rare, it will take as many as possible.\n",
    "    \"\"\"\n",
    "    if len(df) <= n:\n",
    "        return df.sample(frac=1.0, random_state=229).reset_index(drop=True)\n",
    "\n",
    "    df_pos = df[df[label_col] == 1]\n",
    "    df_neg = df[df[label_col] == 0]\n",
    "\n",
    "    # Target roughly balanced, but limited by rare class\n",
    "    half = n // 2\n",
    "    n_pos = min(len(df_pos), half)\n",
    "    n_neg = min(len(df_neg), n - n_pos)\n",
    "\n",
    "    # If not enough negatives/positives, fill from the other\n",
    "    remaining = n - (n_pos + n_neg)\n",
    "    if remaining > 0:\n",
    "        if len(df_pos) - n_pos > len(df_neg) - n_neg:\n",
    "            extra = df_pos.sample(n=min(remaining, len(df_pos) - n_pos), random_state=229)\n",
    "        else:\n",
    "            extra = df_neg.sample(n=min(remaining, len(df_neg) - n_neg), random_state=229)\n",
    "        sampled = pd.concat([\n",
    "            df_pos.sample(n=n_pos, random_state=229),\n",
    "            df_neg.sample(n=n_neg, random_state=229),\n",
    "            extra\n",
    "        ], ignore_index=True)\n",
    "    else:\n",
    "        sampled = pd.concat([\n",
    "            df_pos.sample(n=n_pos, random_state=229),\n",
    "            df_neg.sample(n=n_neg, random_state=229),\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    sampled = sampled.sample(frac=1.0, random_state=229).reset_index(drop=True)\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f2716",
   "metadata": {},
   "source": [
    "## Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17f5e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_translation_wmt_mqm_non_en_to_en(\n",
    "    n_target: int = 800,\n",
    "    seed: int = 229,\n",
    "    group_by=(\"lp\", \"year\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Translation dataset with:\n",
    "      input  = non-English source text\n",
    "      output = English MT output\n",
    "\n",
    "    Filters to lp like 'de-en', 'zh-en', ... (i.e., target is English, source is not).\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"RicardoRei/wmt-mqm-human-evaluation\")\n",
    "    df = ds[\"train\"].to_pandas()\n",
    "\n",
    "    # Keep only non-English -> English pairs\n",
    "    # e.g., 'de-en', 'ru-en', 'zh-en', ...\n",
    "    df = df[df[\"lp\"].str.endswith(\"-en\") & ~df[\"lp\"].str.startswith(\"en-\")].copy()\n",
    "\n",
    "    # Minimal columns\n",
    "    df[\"input\"] = df[\"src\"].map(normalize_ws)   # non-English\n",
    "    df[\"output\"] = df[\"mt\"].map(normalize_ws)   # English\n",
    "    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "\n",
    "    df = df[(df[\"input\"].str.len() > 0) & (df[\"output\"].str.len() > 0)]\n",
    "    df = df.dropna(subset=[\"score\"])\n",
    "\n",
    "    # Median split within (lp, year) => pass/fail without picking a numeric threshold\n",
    "    if group_by:\n",
    "        med = df.groupby(list(group_by))[\"score\"].transform(\"median\")\n",
    "    else:\n",
    "        med = df[\"score\"].median()\n",
    "    df[\"label\"] = (df[\"score\"] >= med).astype(int)\n",
    "\n",
    "    df_min = df[[\"input\", \"output\", \"label\"]]\n",
    "\n",
    "    # Sample\n",
    "    if len(df_min) > n_target:\n",
    "        df_min = df_min.sample(n=n_target, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        df_min = df_min.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    return df_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e405e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1) Translation dataset from MQM (human error annotations)\n",
    "#    PASS iff no Major/Critical errors\n",
    "#    Source: alconost/mqm-translation-gold\n",
    "# ============================================\n",
    "\n",
    "def build_translation_mqm_dataset(\n",
    "    n_target: int = 800,\n",
    "    seed: int = 229\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds a minimal dataset with columns: input, output, label\n",
    "    label = 1 iff MQM annotations contain no Major/Critical errors.\n",
    "    \"\"\"\n",
    "    RNG.seed(seed)\n",
    "\n",
    "    # Load MQM dataset\n",
    "    # Dataset page describes JSONL/TSV structure and MQM fields. :contentReference[oaicite:3]{index=3}\n",
    "    # ds = load_dataset(\"alconost/mqm-translation-gold\")\n",
    "    ds = load_dataset(\"RicardoRei/wmt-mqm-human-evaluation\")\n",
    "\n",
    "    # Some datasets only have a single split; handle robustly.\n",
    "    split_name = \"train\" if \"train\" in ds else list(ds.keys())[0]\n",
    "    data = ds[split_name]\n",
    "\n",
    "    rows = []\n",
    "    for ex in tqdm(data, desc=\"Processing MQM examples\"):\n",
    "        # We will be defensive about field names because HF datasets vary.\n",
    "        # The dataset page documents fields; if any mismatch occurs, inspect first 1-2 rows.\n",
    "        src = ex.get(\"source\") or ex.get(\"src\") or ex.get(\"source_text\") or \"\"\n",
    "        mt = ex.get(\"translation\") or ex.get(\"hypothesis\") or ex.get(\"output\") or ex.get(\"target\") or \"\"\n",
    "        src = normalize_whitespace(src)\n",
    "        mt = normalize_whitespace(mt)\n",
    "\n",
    "        # MQM errors are typically provided as a list/array of annotations\n",
    "        # Each annotation includes severity: Minor/Major/Critical\n",
    "        errors = ex.get(\"errors\") or ex.get(\"mqm_errors\") or ex.get(\"annotations\") or []\n",
    "\n",
    "        major_or_critical = False\n",
    "        if isinstance(errors, list):\n",
    "            for e in errors:\n",
    "                # e might be dict with key 'severity' or similar\n",
    "                if isinstance(e, dict):\n",
    "                    sev = (e.get(\"severity\") or e.get(\"Severity\") or \"\").strip().lower()\n",
    "                    if sev in {\"major\", \"critical\"}:\n",
    "                        major_or_critical = True\n",
    "                        break\n",
    "\n",
    "        label = 0 if major_or_critical else 1\n",
    "\n",
    "        if src and mt:\n",
    "            rows.append({\"input\": src, \"output\": mt, \"label\": int(label)})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Sample to size with decent label mix\n",
    "    df = stratified_sample(df, n=min(n_target, 1000), label_col=\"label\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552c2bd",
   "metadata": {},
   "source": [
    "## Summarization Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da63de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def build_summarization_frank_minimal(\n",
    "    frank_data_dir: str = \"./../../frank/data\",\n",
    "    split: str = \"test\",          # \"test\" | \"validation\" | \"train\" | \"all\"\n",
    "    n_target: int = 800,\n",
    "    seed: int = 229\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build FRANK summarization dataset with exactly:\n",
    "      input  = article\n",
    "      output = summary\n",
    "      label  = pass/fail from human 'Factuality'\n",
    "    Uses human_annotations.json for labels (human annotated).\n",
    "    \"\"\"\n",
    "    path = f\"{frank_data_dir}/human_annotations.json\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for ex in data:\n",
    "        if split != \"all\":\n",
    "            if ex.get(\"split\") != split:\n",
    "                continue\n",
    "\n",
    "        # human_annotations.json does NOT include article/summary text in your peek;\n",
    "        # it includes hash + model_name + labels. So we need benchmark_data.json for text.\n",
    "        # We'll join on (hash, model_name, split).\n",
    "        # (Your peek shows benchmark_data.json has those + article/summary.)\n",
    "        rows.append({\n",
    "            \"hash\": str(ex.get(\"hash\")),\n",
    "            \"model_name\": str(ex.get(\"model_name\")),\n",
    "            \"split\": ex.get(\"split\"),\n",
    "            \"Factuality\": ex.get(\"Factuality\")\n",
    "        })\n",
    "\n",
    "    ann = pd.DataFrame(rows)\n",
    "    if ann.empty:\n",
    "        raise RuntimeError(f\"No FRANK annotations found for split={split} at {path}\")\n",
    "\n",
    "    # Load benchmark_data.json for article/summary\n",
    "    bench_path = f\"{frank_data_dir}/benchmark_data.json\"\n",
    "    with open(bench_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        bench = json.load(f)\n",
    "\n",
    "    bench_rows = []\n",
    "    for ex in bench:\n",
    "        if split != \"all\":\n",
    "            if ex.get(\"split\") != split:\n",
    "                continue\n",
    "        bench_rows.append({\n",
    "            \"hash\": str(ex.get(\"hash\")),\n",
    "            \"model_name\": str(ex.get(\"model_name\")),\n",
    "            \"split\": ex.get(\"split\"),\n",
    "            \"article\": normalize_ws(ex.get(\"article\", \"\")),\n",
    "            \"summary\": normalize_ws(ex.get(\"summary\", \"\")),\n",
    "        })\n",
    "\n",
    "    bench_df = pd.DataFrame(bench_rows)\n",
    "    if bench_df.empty:\n",
    "        raise RuntimeError(f\"No FRANK benchmark rows found for split={split} at {bench_path}\")\n",
    "\n",
    "    # Join (hash, model_name, split)\n",
    "    df = ann.merge(bench_df, on=[\"hash\", \"model_name\", \"split\"], how=\"inner\")\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\n",
    "            \"FRANK join produced 0 rows. \"\n",
    "            \"Check that 'hash' and 'model_name' match between human_annotations.json and benchmark_data.json.\"\n",
    "        )\n",
    "\n",
    "    # Construct minimal columns\n",
    "    # In FRANK, Factuality is a human label; treat 1.0 as pass, 0.0 as fail.\n",
    "    # If it contains floats, coerce safely.\n",
    "    def to_label(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        try:\n",
    "            return 1 if float(x) >= 1.0 else 0\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df[\"label\"] = df[\"Factuality\"].apply(to_label)\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "    df_min = df.rename(columns={\"article\": \"input\", \"summary\": \"output\"})[[\"input\", \"output\", \"label\"]]\n",
    "\n",
    "    # Drop empty text rows\n",
    "    df_min = df_min[(df_min[\"input\"].str.len() > 0) & (df_min[\"output\"].str.len() > 0)]\n",
    "\n",
    "    # Sample to target size\n",
    "    if len(df_min) > n_target:\n",
    "        df_min = df_min.sample(n=n_target, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        df_min = df_min.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    return df_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533ecac",
   "metadata": {},
   "source": [
    "## Extraction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78d8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3) Extraction dataset (NER) with gold human annotations\n",
    "#\n",
    "# We will:\n",
    "#   - load CoNLL-2003 (gold labels are human annotated)\n",
    "#   - generate an \"output\" as predicted entity list using a pretrained NER model\n",
    "#   - label PASS iff predicted entities exactly match gold entities (no numeric thresholds)\n",
    "#\n",
    "# Dataset: eriktks/conll2003 :contentReference[oaicite:5]{index=5}\n",
    "# ============================================\n",
    "\n",
    "def conll_tags_to_entities(tokens: List[str], ner_tags: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Convert BIO tags into a list of (entity_text, entity_type).\n",
    "    ner_tags in CoNLL are typically like 'B-PER', 'I-ORG', 'O', etc.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    cur_tokens = []\n",
    "    cur_type = None\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur_tokens, cur_type\n",
    "        if cur_tokens and cur_type:\n",
    "            entities.append((\" \".join(cur_tokens), cur_type))\n",
    "        cur_tokens, cur_type = [], None\n",
    "\n",
    "    for tok, tag in zip(tokens, ner_tags):\n",
    "        if tag == \"O\" or tag is None:\n",
    "            flush()\n",
    "            continue\n",
    "        if tag.startswith(\"B-\"):\n",
    "            flush()\n",
    "            cur_type = tag[2:]\n",
    "            cur_tokens = [tok]\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            t = tag[2:]\n",
    "            if cur_type == t and cur_tokens:\n",
    "                cur_tokens.append(tok)\n",
    "            else:\n",
    "                # ill-formed, start new\n",
    "                flush()\n",
    "                cur_type = t\n",
    "                cur_tokens = [tok]\n",
    "        else:\n",
    "            # unknown tag format\n",
    "            flush()\n",
    "\n",
    "    flush()\n",
    "    return entities\n",
    "\n",
    "\n",
    "def format_entities_as_string(entities: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Create a stable string representation for entity lists.\n",
    "    \"\"\"\n",
    "    # Sort for deterministic comparison (CoNLL tags are order-preserving but predicted outputs might not be).\n",
    "    ents = sorted([(normalize_whitespace(e), t) for e, t in entities], key=lambda x: (x[1], x[0]))\n",
    "    return json.dumps([{\"text\": e, \"type\": t} for e, t in ents], ensure_ascii=False)\n",
    "\n",
    "\n",
    "def build_extraction_conll_dataset(\n",
    "    n_target: int = 800,\n",
    "    seed: int = 229,\n",
    "    hf_ner_model: str = \"dslim/bert-base-NER\",  # lightweight baseline\n",
    "    max_examples_to_scan: int = 5000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build (input, output, label) for NER extraction:\n",
    "    - input = sentence text\n",
    "    - output = predicted entity list (json string)\n",
    "    - label = 1 iff exact match to gold entity list\n",
    "    \"\"\"\n",
    "    RNG.seed(seed)\n",
    "\n",
    "    ds = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "    split = \"validation\" if \"validation\" in ds else \"test\" if \"test\" in ds else \"train\"\n",
    "    data = ds[split]\n",
    "\n",
    "    # Load NER pipeline\n",
    "    from transformers import pipeline\n",
    "    ner_pipe = pipeline(\"token-classification\", model=hf_ner_model, aggregation_strategy=\"simple\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Scan examples until we have enough rows; exact-match positives can be rare\n",
    "    # so we allow scanning more than n_target.\n",
    "    for i, ex in enumerate(tqdm(data, desc=f\"Processing CoNLL-2003 ({split})\")):\n",
    "        if i >= max_examples_to_scan and len(rows) >= n_target:\n",
    "            break\n",
    "\n",
    "        tokens = ex[\"tokens\"]\n",
    "        # conll2003 provides numeric tags; map to string names\n",
    "        # Feature might be 'ner_tags' (ints) with 'features' mapping\n",
    "        tag_ids = ex[\"ner_tags\"]\n",
    "        tag_names = [data.features[\"ner_tags\"].feature.names[t] for t in tag_ids]\n",
    "\n",
    "        sentence = \" \".join(tokens)\n",
    "        sentence = normalize_whitespace(sentence)\n",
    "\n",
    "        gold_entities = conll_tags_to_entities(tokens, tag_names)\n",
    "        gold_str = format_entities_as_string(gold_entities)\n",
    "\n",
    "        # Model prediction\n",
    "        pred = ner_pipe(sentence)\n",
    "        # pred items have entity_group + word; 'simple' aggregation gives grouped entities\n",
    "        pred_entities = []\n",
    "        for p in pred:\n",
    "            etype = p.get(\"entity_group\") or p.get(\"entity\") or \"\"\n",
    "            text = p.get(\"word\") or \"\"\n",
    "            if etype and text:\n",
    "                # Map common model labels to CoNLL types if needed\n",
    "                # dslim/bert-base-NER uses PER/ORG/LOC/MISC already\n",
    "                pred_entities.append((normalize_whitespace(text), etype))\n",
    "\n",
    "        pred_str = format_entities_as_string(pred_entities)\n",
    "\n",
    "        label = int(pred_str == gold_str)\n",
    "\n",
    "        rows.append({\n",
    "            \"input\": sentence,\n",
    "            \"output\": pred_str,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # For extraction, positives can be very rare. We don't force balance too hard;\n",
    "    # we sample up to target size with whatever mix we have.\n",
    "    if len(df) > n_target:\n",
    "        df = df.sample(n=n_target, random_state=229).reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.sample(frac=1.0, random_state=229).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fe6f7",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b400451",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_N = 800  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f032c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n",
      "label\n",
      "1    458\n",
      "0    342\n",
      "Name: count, dtype: int64\n",
      "波黑奥委会主席：坚信中国有能力在特殊时期办好冬奥盛会-新华网\n",
      "Chairman of the Olympic Committee of Bosnia and Herzegovina: firmly confident that China has the ability to hold the Win\n"
     ]
    }
   ],
   "source": [
    "# 1) Translation (MQM)\n",
    "df_trans = build_translation_wmt_mqm_non_en_to_en(n_target=TARGET_N)\n",
    "print(df_trans.shape)\n",
    "print(df_trans[\"label\"].value_counts())\n",
    "print(df_trans.iloc[0][\"input\"][:120])\n",
    "print(df_trans.iloc[0][\"output\"][:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a5ec6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>波黑奥委会主席：坚信中国有能力在特殊时期办好冬奥盛会-新华网</td>\n",
       "      <td>Chairman of the Olympic Committee of Bosnia an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>兰花：至少有 两万种兰花 -- 多种多样的令人惊异。</td>\n",
       "      <td>Orchids: There are at least 20,000 species of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>当然，你会无助地发现你的头也被挤压和拉伸， 所以你可能无法理解究竟发生了什么。</td>\n",
       "      <td>Now of course, your head would be squeezed and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“很多疾病早期检测、切除不到位，跟医生手术室用的灯有关 。 ”</td>\n",
       "      <td>“The lack of early detection and resection of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>还有一部分品牌，为了推广自己的产品，不择手段，跟医院的医生挂钩，由他们协助产品的推广，从一听...</td>\n",
       "      <td>There are also some brands, in order to promot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                     波黑奥委会主席：坚信中国有能力在特殊时期办好冬奥盛会-新华网   \n",
       "1                         兰花：至少有 两万种兰花 -- 多种多样的令人惊异。   \n",
       "2            当然，你会无助地发现你的头也被挤压和拉伸， 所以你可能无法理解究竟发生了什么。   \n",
       "3                    “很多疾病早期检测、切除不到位，跟医生手术室用的灯有关 。 ”   \n",
       "4  还有一部分品牌，为了推广自己的产品，不择手段，跟医院的医生挂钩，由他们协助产品的推广，从一听...   \n",
       "\n",
       "                                              output  label  \n",
       "0  Chairman of the Olympic Committee of Bosnia an...      1  \n",
       "1  Orchids: There are at least 20,000 species of ...      1  \n",
       "2  Now of course, your head would be squeezed and...      0  \n",
       "3  “The lack of early detection and resection of ...      0  \n",
       "4  There are also some brands, in order to promot...      0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5798d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved translation dataset: ./evals_benchmark_datasets/translation_mqm_input_output_label.csv\n",
      "label\n",
      "1    458\n",
      "0    342\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path_trans = save_minimal_dataset(df_trans, \"translation_mqm_input_output_label\")\n",
    "print(\"Saved translation dataset:\", path_trans)\n",
    "print(df_trans[\"label\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bf182f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FRANK (HF mirror: mtc/frank-test-set-with-faithfulness-annotation): 100%|██████████| 1575/1575 [00:00<00:00, 4377.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2) Summarization (FRANK)\n",
    "config = FrankConfig(\n",
    "    local_json_path=\"./frank/data/<ACTUAL_FILENAME>.json\"\n",
    ")\n",
    "\n",
    "df_sum = build_summarization_frank_dataset(\n",
    "    n_target=TARGET_N,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65dbffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n",
      "label\n",
      "0    520\n",
      "1    280\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France's Dubuisson carded a 67 to tie with ove...</td>\n",
       "      <td>rory mcilroy will take a one-shot lead into th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Share this withEmailFacebookMessengerMessenger...</td>\n",
       "      <td>a man has been found guilty of the murder of a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homeless people in the Bay Area are being hand...</td>\n",
       "      <td>community technology alliance is giving away f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crowds who turned out for the Anzac Day memori...</td>\n",
       "      <td>a contractor doing a sound check inside the ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)Two Transportation Security Administratio...</td>\n",
       "      <td>screeners have been fired after conspiring to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  France's Dubuisson carded a 67 to tie with ove...   \n",
       "1  Share this withEmailFacebookMessengerMessenger...   \n",
       "2  Homeless people in the Bay Area are being hand...   \n",
       "3  Crowds who turned out for the Anzac Day memori...   \n",
       "4  (CNN)Two Transportation Security Administratio...   \n",
       "\n",
       "                                              output  label  \n",
       "0  rory mcilroy will take a one-shot lead into th...      0  \n",
       "1  a man has been found guilty of the murder of a...      1  \n",
       "2  community technology alliance is giving away f...      1  \n",
       "3  a contractor doing a sound check inside the ex...      1  \n",
       "4  screeners have been fired after conspiring to ...      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum = build_summarization_frank_minimal(\n",
    "    frank_data_dir=\"./../../frank/data\",\n",
    "    split=\"test\",\n",
    "    n_target=800\n",
    ")\n",
    "\n",
    "print(df_sum.shape)\n",
    "print(df_sum[\"label\"].value_counts())\n",
    "df_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "111f377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summarization dataset: ./evals_benchmark_datasets/summarization_frank_input_output_label.csv\n"
     ]
    }
   ],
   "source": [
    "path_sum = save_minimal_dataset(df_sum, \"summarization_frank_input_output_label\")\n",
    "print(\"Saved summarization dataset:\", path_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06b8b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ac1d4e64d747d89be59378ceaa9f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d63d50e43b4822ad9846bf3311a25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec14ff5a1a17450e88768978f827f01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a59cf74c7b45c98e001fa34052151e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ed7bb2e044361a23fa3909cfd80d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b12dc7af02e49ab965e1568e6d90e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6623d55dbd4b495eb4d9a3d7cbb91c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b7e01b796f46c3a5832b76491da0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1ad70748fd4dfc8000ecf6131d5411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Processing CoNLL-2003 (validation): 100%|██████████| 3250/3250 [02:06<00:00, 25.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3) Extraction (CoNLL-2003 NER)\n",
    "df_extr = build_extraction_conll_dataset(n_target=TARGET_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f336ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n",
      "label\n",
      "1    505\n",
      "0    295\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Briton , who lost his World Boxing Council...</td>\n",
       "      <td>[{\"text\": \"##riton\", \"type\": \"MISC\"}, {\"text\":...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coach Berti Vogts has called up a virtually id...</td>\n",
       "      <td>[{\"text\": \"Germany\", \"type\": \"LOC\"}, {\"text\": ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday , the Dow Jones index closed down 31...</td>\n",
       "      <td>[{\"text\": \"Dow Jones\", \"type\": \"MISC\"}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOLF - LEADING SCORES AT GREATER MILWAUKEE OPEN .</td>\n",
       "      <td>[{\"text\": \"GREAT\", \"type\": \"MISC\"}, {\"text\": \"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EUROBONDS - Primary market activity was sharpl...</td>\n",
       "      <td>[{\"text\": \"U. S\", \"type\": \"LOC\"}, {\"text\": \"U....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  The Briton , who lost his World Boxing Council...   \n",
       "1  Coach Berti Vogts has called up a virtually id...   \n",
       "2  On Friday , the Dow Jones index closed down 31...   \n",
       "3  GOLF - LEADING SCORES AT GREATER MILWAUKEE OPEN .   \n",
       "4  EUROBONDS - Primary market activity was sharpl...   \n",
       "\n",
       "                                              output  label  \n",
       "0  [{\"text\": \"##riton\", \"type\": \"MISC\"}, {\"text\":...      0  \n",
       "1  [{\"text\": \"Germany\", \"type\": \"LOC\"}, {\"text\": ...      1  \n",
       "2            [{\"text\": \"Dow Jones\", \"type\": \"MISC\"}]      1  \n",
       "3  [{\"text\": \"GREAT\", \"type\": \"MISC\"}, {\"text\": \"...      0  \n",
       "4  [{\"text\": \"U. S\", \"type\": \"LOC\"}, {\"text\": \"U....      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_extr.shape)\n",
    "print(df_extr[\"label\"].value_counts())\n",
    "df_extr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "284bd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extraction dataset: ./evals_benchmark_datasets/extraction_conll_ner_input_output_label.csv\n",
      "label\n",
      "1    505\n",
      "0    295\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path_extr = save_minimal_dataset(df_extr, \"extraction_conll_ner_input_output_label\")\n",
    "print(\"Saved extraction dataset:\", path_extr)\n",
    "print(df_extr[\"label\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1aa5bb",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e3903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Unit-test style guideline specs (text only) so we can instantiate with any judge model\n",
    "# =============================================================================\n",
    "\n",
    "TRANSLATION_SPECS = {\n",
    "    \"meaning_preservation\": [\n",
    "        \"The English output preserves the meaning and intent of the non-English input at the sentence and document level.\",\n",
    "    ],\n",
    "    \"no_added_info\": [\n",
    "        \"The output does not introduce facts, entities, or details that are not present or implied in the input.\",\n",
    "    ],\n",
    "    \"coverage\": [\n",
    "        \"All important details from the input are included (key claims, qualifiers, and relationships).\",\n",
    "    ],\n",
    "    \"named_entities_numbers\": [\n",
    "        \"Proper nouns, dates, numbers, currencies, and units are translated/transcribed correctly without changing values.\",\n",
    "    ],\n",
    "    \"terminology_consistency\": [\n",
    "        \"Repeated terms and named entities are translated consistently throughout the output.\",\n",
    "    ],\n",
    "    \"fluency\": [\n",
    "        \"The output is grammatical, idiomatic English with natural phrasing and correct syntax.\",\n",
    "    ],\n",
    "    \"style_register\": [\n",
    "        \"The output uses an appropriate tone/register for the input (e.g., news-like text remains formal and neutral).\",\n",
    "    ],\n",
    "    \"formatting\": [\n",
    "        \"Punctuation, capitalization, and structural formatting (lists/headers/line breaks) are appropriate and consistent.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "SUMMARIZATION_SPECS = {\n",
    "    \"salience\": [\n",
    "        \"The summary captures the central topic and main takeaway(s) of the source text.\",\n",
    "    ],\n",
    "    \"coverage\": [\n",
    "        \"The summary includes key details needed to understand the main takeaway(s).\",\n",
    "    ],\n",
    "    \"faithfulness\": [\n",
    "        \"All claims are supported by the source; no hallucinated or unsupported facts are introduced.\",\n",
    "    ],\n",
    "    \"attribution_specificity\": [\n",
    "        \"The summary avoids over-specific or overly certain claims unless explicitly supported by the source.\",\n",
    "    ],\n",
    "    \"entity_number_fidelity\": [\n",
    "        \"Names, numbers, dates, locations, and attributions match the source without distortion.\",\n",
    "    ],\n",
    "    \"coherence\": [\n",
    "        \"The summary is logically ordered and easy to follow with clear sentence-to-sentence flow.\",\n",
    "    ],\n",
    "    \"conciseness\": [\n",
    "        \"The summary is succinct with minimal redundancy and no filler.\",\n",
    "    ],\n",
    "    \"non_contradiction\": [\n",
    "        \"The summary does not contradict the source or itself.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "EXTRACTION_SPECS = {\n",
    "    \"schema_format\": [\n",
    "        \"The output strictly follows the expected schema/format and uses correct data types for each field.\",\n",
    "    ],\n",
    "    \"no_hallucinations\": [\n",
    "        \"The output does not include entities/values that are not present in the input text.\",\n",
    "    ],\n",
    "    \"span_grounding\": [\n",
    "        \"Extracted strings are grounded in the input text (verbatim spans or faithful aliases).\",\n",
    "    ],\n",
    "    \"type_correctness\": [\n",
    "        \"Each extracted item has the correct type/category (e.g., PER/ORG/LOC/MISC or your task's schema types).\",\n",
    "    ],\n",
    "    \"completeness_recall\": [\n",
    "        \"All relevant entities/values present in the input are extracted; no major items are missing.\",\n",
    "    ],\n",
    "    \"dedup_normalization\": [\n",
    "        \"Entities/values are normalized consistently (casing/whitespace) and duplicates are removed appropriately.\",\n",
    "    ],\n",
    "    \"boundary_precision\": [\n",
    "        \"Entity boundaries are precise (not overly broad/narrow); extracted spans correspond to the intended entity mention.\",\n",
    "    ],\n",
    "    \"consistency\": [\n",
    "        \"The output is internally consistent (the same entity/value is not represented in conflicting ways).\",\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Databricks connection & MLflow tracing\n",
    "# ---------------------------------------------------------------------------\n",
    "# MLflow tracking\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"\"\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"\"\n",
    "os.environ[\"MLFLOW_REGISTRY_URI\"] = \"\"\n",
    "os.environ[\"MLFLOW_EXPERIMENT_ID\"] = \"\"\n",
    "\n",
    "# API keys for models (Hidden for public access)\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"\"\n",
    "JUDGE_MODEL = \"openrouter:/anthropic/claude-3.5-sonnet\"\n",
    "\n",
    "# alternatives:\n",
    "# JUDGE_MODEL = \"openrouter:/openai/gpt-4\"\n",
    "# JUDGE_MODEL = \"openrouter:/openai/gpt-4o-mini\"\n",
    "# JUDGE_MODEL = \"openrouter:/meta-llama/llama-3.1-405b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588f7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.dspy.autolog(log_traces=True, log_traces_from_eval=True, log_compiles=True, log_evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "y3ply4wsqz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation scorers:   ['meaning_preservation', 'no_added_info', 'coverage', 'named_entities_numbers', 'terminology_consistency', 'fluency', 'style_register', 'formatting']\n",
      "Summarization scorers: ['salience', 'coverage', 'faithfulness', 'attribution_specificity', 'entity_number_fidelity', 'coherence', 'conciseness', 'non_contradiction']\n",
      "Extraction scorers:    ['schema_format', 'no_hallucinations', 'span_grounding', 'type_correctness', 'completeness_recall', 'dedup_normalization', 'boundary_precision', 'consistency']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Create scorer objects from the spec dicts defined above\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def specs_to_scorers(specs: dict, model: str) -> list:\n",
    "    \"\"\"Convert {name: [guidelines]} dict to list of MLflow Guidelines scorers.\"\"\"\n",
    "    return [\n",
    "        make_guidelines_scorer(name=name, guidelines=guidelines, model=model)\n",
    "        for name, guidelines in specs.items()\n",
    "    ]\n",
    "\n",
    "translation_scorers = specs_to_scorers(TRANSLATION_SPECS, JUDGE_MODEL)\n",
    "summarization_scorers = specs_to_scorers(SUMMARIZATION_SPECS, JUDGE_MODEL)\n",
    "extraction_scorers = specs_to_scorers(EXTRACTION_SPECS, JUDGE_MODEL)\n",
    "\n",
    "print(f\"Translation scorers:   {[s.name for s in translation_scorers]}\")\n",
    "print(f\"Summarization scorers: {[s.name for s in summarization_scorers]}\")\n",
    "print(f\"Extraction scorers:    {[s.name for s in extraction_scorers]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "djrmixqw0rq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Helper: convert DataFrame rows → MLflow eval data (with pre-existing outputs)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def df_to_eval_data(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Convert DataFrame to MLflow eval data with pre-existing outputs.\n",
    "\n",
    "    Uses 'inputs' for the source text and 'outputs' for the model output,\n",
    "    so mlflow.genai.evaluate() scores existing outputs without regeneration.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"inputs\": {\"question\": row[\"input\"]},\n",
    "            \"outputs\": {\"response\": row[\"output\"]},\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Core scoring function\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_judge_scoring(\n",
    "    df: pd.DataFrame,\n",
    "    scorers: list,\n",
    "    task_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Score all rows with LLM judge and return df with new feature columns.\n",
    "\n",
    "    Each Guidelines scorer produces a yes/no verdict per row, which is\n",
    "    converted to 1/0 and added as a new column.\n",
    "    \"\"\"\n",
    "    eval_data = df_to_eval_data(df)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{task_name}_judge_scoring\"):\n",
    "        result = mlflow.genai.evaluate(\n",
    "            data=eval_data,\n",
    "            scorers=scorers,\n",
    "        )\n",
    "\n",
    "    # Extract per-row results\n",
    "    eval_table = result.result_df\n",
    "\n",
    "    # Debug: show available columns so we can identify the naming pattern\n",
    "    print(f\"[{task_name}] Eval table shape: {eval_table.shape}\")\n",
    "    print(f\"[{task_name}] Eval table columns: {list(eval_table.columns)}\")\n",
    "\n",
    "    # Map yes/no → 1/0 for each scorer\n",
    "    df_features = df.copy().reset_index(drop=True)\n",
    "    for scorer in scorers:\n",
    "        # MLflow version may use different column name patterns\n",
    "        for col_pattern in [f\"{scorer.name}/value\"]:\n",
    "            if col_pattern in eval_table.columns:\n",
    "                df_features[scorer.name] = (\n",
    "                    eval_table[col_pattern]\n",
    "                    .map({\"yes\": 1, \"no\": 0})\n",
    "                    .fillna(0)\n",
    "                    .astype(int)\n",
    "                    .values\n",
    "                )\n",
    "                break\n",
    "\n",
    "    added_cols = [c for c in df_features.columns if c not in [\"input\", \"output\", \"label\"]]\n",
    "    print(f\"[{task_name}] Added {len(added_cols)} feature(s): {added_cols}\")\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6r38hiy0l",
   "metadata": {},
   "source": [
    "## Small-Scale Validation (10 rows of summarization)\n",
    "\n",
    "Run this first to validate that scorers work, features appear, and Databricks traces look correct.\n",
    "After confirming, proceed to the full pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0x6rz6f00h8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 20 rows\n",
      "Label distribution: {0: 10, 1: 10}\n"
     ]
    }
   ],
   "source": [
    "# Load summarization dataset\n",
    "df_sum = pd.read_csv(f\"{OUT_DIR}/summarization_frank_input_output_label.csv\")\n",
    "\n",
    "# Take 10 rows: 5 label=0 + 5 label=1\n",
    "df_test = pd.concat([\n",
    "    df_sum[df_sum[\"label\"] == 0].head(10),\n",
    "    df_sum[df_sum[\"label\"] == 1].head(10),\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Test set: {len(df_test)} rows\")\n",
    "print(f\"Label distribution: {df_test['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1583e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France's Dubuisson carded a 67 to tie with ove...</td>\n",
       "      <td>rory mcilroy will take a one-shot lead into th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)Two Transportation Security Administratio...</td>\n",
       "      <td>screeners have been fired after conspiring to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marcy Smith was woken up by her son David to f...</td>\n",
       "      <td>the family of a man who died in a blaze at her...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan Walls took pictures of 101 passengers dur...</td>\n",
       "      <td>an edinburgh taxi driver who took thousands of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It works by looking for a combination of \"mark...</td>\n",
       "      <td>an international test for alzheimer\\'s disease...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Share this withEmailFacebookMessengerMessenger...</td>\n",
       "      <td>a man has been taken to hospital after a crash...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Share this withEmailFacebookMessengerMessenger...</td>\n",
       "      <td>the former leader of birmingham city council h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The shooting occurred at a hostel attached to ...</td>\n",
       "      <td>a 19-year-old indian student has been shot dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Share this withEmailFacebookTwitterWhatsAppLin...</td>\n",
       "      <td>it\\'s a tale of the indian matchbox industry, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Denise Fergus said she had been \"let down so m...</td>\n",
       "      <td>the mother of murdered toddler james bulger ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Share this withEmailFacebookMessengerMessenger...</td>\n",
       "      <td>a man has been found guilty of the murder of a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Homeless people in the Bay Area are being hand...</td>\n",
       "      <td>community technology alliance is giving away f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Crowds who turned out for the Anzac Day memori...</td>\n",
       "      <td>a contractor doing a sound check inside the ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Richie Benaud has passed away at the age of 84...</td>\n",
       "      <td>richie benaud has passed away at the age of 84...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A 20-year-old British tourist has claimed she ...</td>\n",
       "      <td>a 20-year-old british tourist has accused a ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(CNN)An unmanned Russian spacecraft originally...</td>\n",
       "      <td>the spacecraft will re-enter the earth 's atmo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A midwife has today accepted her mistakes 'con...</td>\n",
       "      <td>marie ratcliffe refused to defend herself agai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hillary Clinton has garnered one of her first ...</td>\n",
       "      <td>the group , UNK , launched UNK on monday to su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>They have become one of the most irritating ac...</td>\n",
       "      <td>hiroshi ueda claims that he first invented the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Masters Wednesday began on a sombre note when ...</td>\n",
       "      <td>marc leishman has withdrawn from this year 's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "0   France's Dubuisson carded a 67 to tie with ove...   \n",
       "1   (CNN)Two Transportation Security Administratio...   \n",
       "2   Marcy Smith was woken up by her son David to f...   \n",
       "3   Ryan Walls took pictures of 101 passengers dur...   \n",
       "4   It works by looking for a combination of \"mark...   \n",
       "5   Share this withEmailFacebookMessengerMessenger...   \n",
       "6   Share this withEmailFacebookMessengerMessenger...   \n",
       "7   The shooting occurred at a hostel attached to ...   \n",
       "8   Share this withEmailFacebookTwitterWhatsAppLin...   \n",
       "9   Denise Fergus said she had been \"let down so m...   \n",
       "10  Share this withEmailFacebookMessengerMessenger...   \n",
       "11  Homeless people in the Bay Area are being hand...   \n",
       "12  Crowds who turned out for the Anzac Day memori...   \n",
       "13  Richie Benaud has passed away at the age of 84...   \n",
       "14  A 20-year-old British tourist has claimed she ...   \n",
       "15  (CNN)An unmanned Russian spacecraft originally...   \n",
       "16  A midwife has today accepted her mistakes 'con...   \n",
       "17  Hillary Clinton has garnered one of her first ...   \n",
       "18  They have become one of the most irritating ac...   \n",
       "19  Masters Wednesday began on a sombre note when ...   \n",
       "\n",
       "                                               output  label  \n",
       "0   rory mcilroy will take a one-shot lead into th...      0  \n",
       "1   screeners have been fired after conspiring to ...      0  \n",
       "2   the family of a man who died in a blaze at her...      0  \n",
       "3   an edinburgh taxi driver who took thousands of...      0  \n",
       "4   an international test for alzheimer\\'s disease...      0  \n",
       "5   a man has been taken to hospital after a crash...      0  \n",
       "6   the former leader of birmingham city council h...      0  \n",
       "7   a 19-year-old indian student has been shot dea...      0  \n",
       "8   it\\'s a tale of the indian matchbox industry, ...      0  \n",
       "9   the mother of murdered toddler james bulger ha...      0  \n",
       "10  a man has been found guilty of the murder of a...      1  \n",
       "11  community technology alliance is giving away f...      1  \n",
       "12  a contractor doing a sound check inside the ex...      1  \n",
       "13  richie benaud has passed away at the age of 84...      1  \n",
       "14  a 20-year-old british tourist has accused a ta...      1  \n",
       "15  the spacecraft will re-enter the earth 's atmo...      1  \n",
       "16  marie ratcliffe refused to defend herself agai...      1  \n",
       "17  the group , UNK , launched UNK on monday to su...      1  \n",
       "18  hiroshi ueda claims that he first invented the...      1  \n",
       "19  marc leishman has withdrawn from this year 's ...      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3ce0dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18679ed25eff4cc1b8fb6742a8896d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://dbc-04ae08e6-abf4.cloud.databricks.com/ml/experiments/2241596453982194/evaluation-runs?selectedRunUuid=511f5a1607aa4c3aad6bf3854c738fd4\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[claude3_5] Eval table shape: (20, 20)\n",
      "[claude3_5] Eval table columns: ['trace_id', 'attribution_specificity/value', 'coverage/value', 'salience/value', 'conciseness/value', 'entity_number_fidelity/value', 'non_contradiction/value', 'faithfulness/value', 'coherence/value', 'trace', 'client_request_id', 'state', 'request_time', 'execution_duration', 'request', 'response', 'trace_metadata', 'tags', 'spans', 'assessments']\n",
      "[claude3_5] Added 8 feature(s): ['salience', 'coverage', 'faithfulness', 'attribution_specificity', 'entity_number_fidelity', 'coherence', 'conciseness', 'non_contradiction']\n"
     ]
    }
   ],
   "source": [
    "# Run scoring on the small set (80 LLM calls: 10 rows x 8 scorers)\n",
    "df_test_scored = run_judge_scoring(df_test, summarization_scorers, \"claude3_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pvmv8b071y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Correlation with ground-truth label ---\n",
      "  salience                        corr = +0.333\n",
      "  coverage                        corr = +0.734\n",
      "  faithfulness                    corr = +0.800\n",
      "  attribution_specificity         corr = +0.816\n",
      "  entity_number_fidelity          corr = +0.734\n",
      "  coherence                       corr = +0.105\n",
      "  conciseness                     corr = +0.000\n",
      "  non_contradiction               corr = +0.816\n",
      "\n",
      "--- Feature means ---\n",
      "  salience                        mean = 0.10\n",
      "  coverage                        mean = 0.35\n",
      "  faithfulness                    mean = 0.50\n",
      "  attribution_specificity         mean = 0.60\n",
      "  entity_number_fidelity          mean = 0.35\n",
      "  coherence                       mean = 0.35\n",
      "  conciseness                     mean = 0.50\n",
      "  non_contradiction               mean = 0.60\n"
     ]
    }
   ],
   "source": [
    "# Inspect the small-scale results\n",
    "feature_cols = [c for c in df_test_scored.columns if c not in [\"input\", \"output\", \"label\"]]\n",
    "\n",
    "print(f\"\\n--- Correlation with ground-truth label ---\")\n",
    "for col in feature_cols:\n",
    "    corr = df_test_scored[\"label\"].corr(df_test_scored[col])\n",
    "    print(f\"  {col:30s}  corr = {corr:+.3f}\")\n",
    "\n",
    "print(f\"\\n--- Feature means ---\")\n",
    "for col in feature_cols:\n",
    "    print(f\"  {col:30s}  mean = {df_test_scored[col].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dmle63z6yn",
   "metadata": {},
   "source": [
    "## Full Pipeline: Score All 3 Datasets (800 rows each)\n",
    "\n",
    "Only run this after validating the small-scale test above.\n",
    "This will make ~19,200 LLM judge calls and take ~30-60 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0h544663nf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:   (800, 3)\n",
      "Summarization: (800, 3)\n",
      "Extraction:    (800, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load all three datasets\n",
    "df_trans = pd.read_csv(f\"{OUT_DIR}/translation_mqm_input_output_label.csv\")\n",
    "df_sum   = pd.read_csv(f\"{OUT_DIR}/summarization_frank_input_output_label.csv\")\n",
    "df_extr  = pd.read_csv(f\"{OUT_DIR}/extraction_conll_ner_input_output_label.csv\")\n",
    "\n",
    "print(f\"Translation:   {df_trans.shape}\")\n",
    "print(f\"Summarization: {df_sum.shape}\")\n",
    "print(f\"Extraction:    {df_extr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed2c62",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3h7wp87t0py",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939cc84d79c045efab640a8d8c15a4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/800 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://dbc-04ae08e6-abf4.cloud.databricks.com/ml/experiments/2241596453982194/evaluation-runs?selectedRunUuid=5dadb487ebb04b2f9f183a95b38a0a9b\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[summarization] Eval table shape: (800, 20)\n",
      "[summarization] Eval table columns: ['trace_id', 'non_contradiction/value', 'conciseness/value', 'coverage/value', 'faithfulness/value', 'salience/value', 'coherence/value', 'entity_number_fidelity/value', 'attribution_specificity/value', 'trace', 'client_request_id', 'state', 'request_time', 'execution_duration', 'request', 'response', 'trace_metadata', 'tags', 'spans', 'assessments']\n",
      "[summarization] Added 8 feature(s): ['salience', 'coverage', 'faithfulness', 'attribution_specificity', 'entity_number_fidelity', 'coherence', 'conciseness', 'non_contradiction']\n"
     ]
    }
   ],
   "source": [
    "# Score each dataset (each run is traced in Databricks)\n",
    "df_sum_scored   = run_judge_scoring(df_sum, summarization_scorers, \"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "o8uipobc7ag",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched datasets:\n",
      "  ./evals_benchmark_datasets/summarization_frank_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Save enriched datasets to new CSVs\n",
    "df_sum_scored.to_csv(f\"{OUT_DIR}/summarization_frank_features.csv\", index=False)\n",
    "\n",
    "print(\"Saved enriched datasets:\")\n",
    "print(f\"  {OUT_DIR}/summarization_frank_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sk6apeuo6ya",
   "metadata": {},
   "source": [
    "## Drop rows with errored LLM judge assessments\n",
    "\n",
    "The Databricks traces UI shows ~22 rows where individual scorer assessments errored.\n",
    "These rows got 0s from `fillna(0)` instead of real scores. Fetch traces from Databricks\n",
    "and drop any row where at least one scorer assessment has an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fmdhbf4x2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 5dadb487ebb04b2f9f183a95b38a0a9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cq/wg8z_06n30j0x8_f029_jw_c0000gn/T/ipykernel_60205/4081167159.py:12: FutureWarning: Parameter 'experiment_ids' is deprecated. Please use 'locations' instead.\n",
      "  traces_df = mlflow.search_traces(\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 9\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 9\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: evrim-databricks-unity-catalog-905418331516.s3.us-east-1.amazonaws.com. Connection pool size: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traces: 800\n",
      "Traces with at least one scorer error: 27\n",
      "\n",
      "request type: <class 'dict'>\n",
      "request sample: {'question': 'Ofsted says it has found evidence of children being taught in squalid conditions in three places in Birmingham which have now closed.Anyone running illegal schools could face a jail term of up to 51 weeks.Ministers are also consulting on plans for more regulation of places teaching for\n"
     ]
    }
   ],
   "source": [
    "# Find the summarization scoring run\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[\"\"],\n",
    "    filter_string='tags.mlflow.runName = \"summarization_judge_scoring\"',\n",
    "    output_format=\"list\",\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "print(f\"Run: {run_id}\")\n",
    "\n",
    "# Fetch all traces — must include spans so the `request` field is populated\n",
    "# (request reads from root span's INPUTS attribute; without spans it's None)\n",
    "traces_df = mlflow.search_traces(\n",
    "    experiment_ids=[\"\"],\n",
    "    run_id=run_id,\n",
    ")\n",
    "print(f\"Traces: {len(traces_df)}\")\n",
    "\n",
    "# Check each trace's assessments for any scorer error\n",
    "def has_any_assessment_error(assessments):\n",
    "    if not assessments:\n",
    "        return True\n",
    "    for a in assessments:\n",
    "        feedback = a.get(\"feedback\", {})\n",
    "        if feedback.get(\"error\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "traces_df[\"has_error\"] = traces_df[\"assessments\"].apply(has_any_assessment_error)\n",
    "print(f\"Traces with at least one scorer error: {traces_df['has_error'].sum()}\")\n",
    "\n",
    "# Diagnostic: show what the request field actually looks like\n",
    "sample_req = traces_df[\"request\"].iloc[0]\n",
    "print(f\"\\nrequest type: {type(sample_req)}\")\n",
    "print(f\"request sample: {str(sample_req)[:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e00ff3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>trace</th>\n",
       "      <th>client_request_id</th>\n",
       "      <th>state</th>\n",
       "      <th>request_time</th>\n",
       "      <th>execution_duration</th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "      <th>trace_metadata</th>\n",
       "      <th>tags</th>\n",
       "      <th>spans</th>\n",
       "      <th>assessments</th>\n",
       "      <th>has_error</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tr-fcc516b007750aeb8d61553b8a4c143d</td>\n",
       "      <td>{\"info\": {\"trace_id\": \"tr-fcc516b007750aeb8d61...</td>\n",
       "      <td>tr-fcc516b007750aeb8d61553b8a4c143d</td>\n",
       "      <td>OK</td>\n",
       "      <td>1771550511878</td>\n",
       "      <td>0</td>\n",
       "      <td>{'question': 'This is the moment a paedophile ...</td>\n",
       "      <td>{'response': 'married adeli , 32 , arranged to...</td>\n",
       "      <td>{'mlflow.source.git.commit': '2d77d10e012ac76e...</td>\n",
       "      <td>{'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...</td>\n",
       "      <td>[{'trace_id': '/MUWsAd1CuuNYVU7ikwUPQ==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-106093c9ed8b428b8033d9a2...</td>\n",
       "      <td>True</td>\n",
       "      <td>This is the moment a paedophile was caught by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>tr-452c716c95402a76b558d725dbf4105e</td>\n",
       "      <td>{\"info\": {\"trace_id\": \"tr-452c716c95402a76b558...</td>\n",
       "      <td>tr-452c716c95402a76b558d725dbf4105e</td>\n",
       "      <td>OK</td>\n",
       "      <td>1771550335092</td>\n",
       "      <td>0</td>\n",
       "      <td>{'question': 'An HIV-positive Ohio man accused...</td>\n",
       "      <td>{'response': 'keith anthony allen , 27 , plead...</td>\n",
       "      <td>{'mlflow.source.git.commit': '2d77d10e012ac76e...</td>\n",
       "      <td>{'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...</td>\n",
       "      <td>[{'trace_id': 'RSxxbJVAKna1WNcl2/QQXg==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-e261ab2ad2c947ceaaef97c1...</td>\n",
       "      <td>True</td>\n",
       "      <td>An HIV-positive Ohio man accused of sexually a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>tr-71619882aa9ad9d0937c90d2d525882a</td>\n",
       "      <td>{\"info\": {\"trace_id\": \"tr-71619882aa9ad9d0937c...</td>\n",
       "      <td>tr-71619882aa9ad9d0937c90d2d525882a</td>\n",
       "      <td>OK</td>\n",
       "      <td>1771550328697</td>\n",
       "      <td>0</td>\n",
       "      <td>{'question': '(CNN)Deputies rushed Kenneth Mor...</td>\n",
       "      <td>{'response': 'UNK is accused of killing an emp...</td>\n",
       "      <td>{'mlflow.source.git.commit': '2d77d10e012ac76e...</td>\n",
       "      <td>{'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...</td>\n",
       "      <td>[{'trace_id': 'cWGYgqqa2dCTfJDS1SWIKg==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-bd0c2e7c0b6e4b989358938a...</td>\n",
       "      <td>True</td>\n",
       "      <td>(CNN)Deputies rushed Kenneth Morgan Stancil II...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>tr-79d98567d2a4490a41ee3c26393c6064</td>\n",
       "      <td>{\"info\": {\"trace_id\": \"tr-79d98567d2a4490a41ee...</td>\n",
       "      <td>tr-79d98567d2a4490a41ee3c26393c6064</td>\n",
       "      <td>OK</td>\n",
       "      <td>1771550325732</td>\n",
       "      <td>0</td>\n",
       "      <td>{'question': 'This is the moment a paedophile ...</td>\n",
       "      <td>{'response': 'UNK UNK , who is of UNK origin ,...</td>\n",
       "      <td>{'mlflow.source.git.commit': '2d77d10e012ac76e...</td>\n",
       "      <td>{'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...</td>\n",
       "      <td>[{'trace_id': 'edmFZ9KkSQpB7jwmOTxgZA==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-57452f735b0c467690050351...</td>\n",
       "      <td>True</td>\n",
       "      <td>This is the moment a paedophile was caught by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>tr-696d53b9a17151d19e47f45e8e54b72c</td>\n",
       "      <td>{\"info\": {\"trace_id\": \"tr-696d53b9a17151d19e47...</td>\n",
       "      <td>tr-696d53b9a17151d19e47f45e8e54b72c</td>\n",
       "      <td>OK</td>\n",
       "      <td>1771550246549</td>\n",
       "      <td>0</td>\n",
       "      <td>{'question': 'It's not intended to be a safety...</td>\n",
       "      <td>{'response': 'footage shows the skater confide...</td>\n",
       "      <td>{'mlflow.source.git.commit': '2d77d10e012ac76e...</td>\n",
       "      <td>{'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...</td>\n",
       "      <td>[{'trace_id': 'aW1TuaFxUdGeR/RejlS3LA==', 'spa...</td>\n",
       "      <td>[{'assessment_id': 'a-2eee1568d9eb43fe95206f7a...</td>\n",
       "      <td>True</td>\n",
       "      <td>It's not intended to be a safety video. But th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                trace_id  \\\n",
       "14   tr-fcc516b007750aeb8d61553b8a4c143d   \n",
       "122  tr-452c716c95402a76b558d725dbf4105e   \n",
       "126  tr-71619882aa9ad9d0937c90d2d525882a   \n",
       "128  tr-79d98567d2a4490a41ee3c26393c6064   \n",
       "184  tr-696d53b9a17151d19e47f45e8e54b72c   \n",
       "\n",
       "                                                 trace  \\\n",
       "14   {\"info\": {\"trace_id\": \"tr-fcc516b007750aeb8d61...   \n",
       "122  {\"info\": {\"trace_id\": \"tr-452c716c95402a76b558...   \n",
       "126  {\"info\": {\"trace_id\": \"tr-71619882aa9ad9d0937c...   \n",
       "128  {\"info\": {\"trace_id\": \"tr-79d98567d2a4490a41ee...   \n",
       "184  {\"info\": {\"trace_id\": \"tr-696d53b9a17151d19e47...   \n",
       "\n",
       "                       client_request_id state   request_time  \\\n",
       "14   tr-fcc516b007750aeb8d61553b8a4c143d    OK  1771550511878   \n",
       "122  tr-452c716c95402a76b558d725dbf4105e    OK  1771550335092   \n",
       "126  tr-71619882aa9ad9d0937c90d2d525882a    OK  1771550328697   \n",
       "128  tr-79d98567d2a4490a41ee3c26393c6064    OK  1771550325732   \n",
       "184  tr-696d53b9a17151d19e47f45e8e54b72c    OK  1771550246549   \n",
       "\n",
       "     execution_duration                                            request  \\\n",
       "14                    0  {'question': 'This is the moment a paedophile ...   \n",
       "122                   0  {'question': 'An HIV-positive Ohio man accused...   \n",
       "126                   0  {'question': '(CNN)Deputies rushed Kenneth Mor...   \n",
       "128                   0  {'question': 'This is the moment a paedophile ...   \n",
       "184                   0  {'question': 'It's not intended to be a safety...   \n",
       "\n",
       "                                              response  \\\n",
       "14   {'response': 'married adeli , 32 , arranged to...   \n",
       "122  {'response': 'keith anthony allen , 27 , plead...   \n",
       "126  {'response': 'UNK is accused of killing an emp...   \n",
       "128  {'response': 'UNK UNK , who is of UNK origin ,...   \n",
       "184  {'response': 'footage shows the skater confide...   \n",
       "\n",
       "                                        trace_metadata  \\\n",
       "14   {'mlflow.source.git.commit': '2d77d10e012ac76e...   \n",
       "122  {'mlflow.source.git.commit': '2d77d10e012ac76e...   \n",
       "126  {'mlflow.source.git.commit': '2d77d10e012ac76e...   \n",
       "128  {'mlflow.source.git.commit': '2d77d10e012ac76e...   \n",
       "184  {'mlflow.source.git.commit': '2d77d10e012ac76e...   \n",
       "\n",
       "                                                  tags  \\\n",
       "14   {'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...   \n",
       "122  {'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...   \n",
       "126  {'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...   \n",
       "128  {'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...   \n",
       "184  {'mlflow.user': 'annie@evrim.ai', 'mlflow.trac...   \n",
       "\n",
       "                                                 spans  \\\n",
       "14   [{'trace_id': '/MUWsAd1CuuNYVU7ikwUPQ==', 'spa...   \n",
       "122  [{'trace_id': 'RSxxbJVAKna1WNcl2/QQXg==', 'spa...   \n",
       "126  [{'trace_id': 'cWGYgqqa2dCTfJDS1SWIKg==', 'spa...   \n",
       "128  [{'trace_id': 'edmFZ9KkSQpB7jwmOTxgZA==', 'spa...   \n",
       "184  [{'trace_id': 'aW1TuaFxUdGeR/RejlS3LA==', 'spa...   \n",
       "\n",
       "                                           assessments  has_error  \\\n",
       "14   [{'assessment_id': 'a-106093c9ed8b428b8033d9a2...       True   \n",
       "122  [{'assessment_id': 'a-e261ab2ad2c947ceaaef97c1...       True   \n",
       "126  [{'assessment_id': 'a-bd0c2e7c0b6e4b989358938a...       True   \n",
       "128  [{'assessment_id': 'a-57452f735b0c467690050351...       True   \n",
       "184  [{'assessment_id': 'a-2eee1568d9eb43fe95206f7a...       True   \n",
       "\n",
       "                                         question_text  \n",
       "14   This is the moment a paedophile was caught by ...  \n",
       "122  An HIV-positive Ohio man accused of sexually a...  \n",
       "126  (CNN)Deputies rushed Kenneth Morgan Stancil II...  \n",
       "128  This is the moment a paedophile was caught by ...  \n",
       "184  It's not intended to be a safety video. But th...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces_df[traces_df['has_error'] == True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "thn5vm5lof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-null question_text: 800/800\n",
      "Unique error input texts: 13\n",
      "Rows matched for removal: 32\n",
      "\n",
      "Cleaned: (768, 11)\n",
      "label\n",
      "0    501\n",
      "1    267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature stats after cleaning ---\n",
      "  salience                        mean=0.16  corr=+0.427\n",
      "  coverage                        mean=0.24  corr=+0.545\n",
      "  faithfulness                    mean=0.42  corr=+0.711\n",
      "  attribution_specificity         mean=0.47  corr=+0.629\n",
      "  entity_number_fidelity          mean=0.27  corr=+0.620\n",
      "  coherence                       mean=0.41  corr=+0.380\n",
      "  conciseness                     mean=0.50  corr=+0.212\n",
      "  non_contradiction               mean=0.43  corr=+0.726\n",
      "\n",
      "Saved: 768 rows (dropped 32 errors)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_question(request_val):\n",
    "    \"\"\"Extract the input text from the trace request field.\"\"\"\n",
    "    if request_val is None:\n",
    "        return None\n",
    "    if isinstance(request_val, str):\n",
    "        try:\n",
    "            request_val = json.loads(request_val)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return None\n",
    "    if isinstance(request_val, dict):\n",
    "        if \"question\" in request_val:\n",
    "            return request_val[\"question\"]\n",
    "        inputs = request_val.get(\"inputs\", {})\n",
    "        if isinstance(inputs, dict) and \"question\" in inputs:\n",
    "            return inputs[\"question\"]\n",
    "    return None\n",
    "\n",
    "traces_df[\"question_text\"] = traces_df[\"request\"].apply(extract_question)\n",
    "print(f\"Non-null question_text: {traces_df['question_text'].notna().sum()}/{len(traces_df)}\")\n",
    "\n",
    "# Collect the input texts of errored traces\n",
    "error_texts = set(\n",
    "    traces_df[traces_df[\"has_error\"]][\"question_text\"].dropna().tolist()\n",
    ")\n",
    "print(f\"Unique error input texts: {len(error_texts)}\")\n",
    "\n",
    "# Filter df_sum_scored by excluding rows whose input text matches an errored trace\n",
    "mask = df_sum_scored[\"input\"].isin(error_texts)\n",
    "print(f\"Rows matched for removal: {mask.sum()}\")\n",
    "\n",
    "df_sum_cleaned = df_sum_scored[~mask].reset_index(drop=True)\n",
    "print(f\"\\nCleaned: {df_sum_cleaned.shape}\")\n",
    "print(df_sum_cleaned[\"label\"].value_counts())\n",
    "\n",
    "scorer_cols = [c for c in df_sum_cleaned.columns if c not in [\"input\", \"output\", \"label\"]]\n",
    "print(f\"\\n--- Feature stats after cleaning ---\")\n",
    "for col in scorer_cols:\n",
    "    corr = df_sum_cleaned[\"label\"].corr(df_sum_cleaned[col])\n",
    "    print(f\"  {col:30s}  mean={df_sum_cleaned[col].mean():.2f}  corr={corr:+.3f}\")\n",
    "\n",
    "df_sum_cleaned.to_csv(f\"{OUT_DIR}/summarization_frank_features.csv\", index=False)\n",
    "print(f\"\\nSaved: {len(df_sum_cleaned)} rows (dropped {mask.sum()} errors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24be67",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trans_scored = run_judge_scoring(df_trans, translation_scorers, \"translation\")\n",
    "# df_extr_scored  = run_judge_scoring(df_extr, extraction_scorers, \"extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trans_scored.to_csv(f\"{OUT_DIR}/translation_mqm_features.csv\", index=False)\n",
    "# df_extr_scored.to_csv(f\"{OUT_DIR}/extraction_conll_ner_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907c7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handshake-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
